{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@shivambansal36/language-modelling-text-generation-using-lstms-deep-learning-for-nlp-ed36b224b275"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\"\n",
    "If you can make one heap of all your winnings\n",
    "And risk it on one turn of pitch-and-toss,\n",
    "And lose, and start again at your beginnings,\n",
    "And never breathe a word about your loss:\n",
    "If you can force your heart and nerve and sinew\n",
    "To serve your turn long after they are gone,\n",
    "And so hold on when there is nothing in you\n",
    "Except the Will which says to them: 'Hold on! '\n",
    "\n",
    "If you can talk with crowds and keep your virtue,\n",
    "Or walk with Kings- -nor lose the common touch,\n",
    "If neither foes nor loving friends can hurt you,\n",
    "If all men count with you, but none too much:\n",
    "If you can fill the unforgiving minute\n",
    "With sixty seconds' worth of distance run,\n",
    "Yours is the Earth and everything that's in it,\n",
    "And- -which is more- -you'll be a Man, my son!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "max_sequence_len = 0\n",
    "\n",
    "def dataset_preparation():\n",
    "    corpus = data.lower().split(\"\\n\")\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    input_sequences = []\n",
    "    \n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequence([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    \n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array([pad_sequences(input_sequences, maxlen=mx_sequence_len, padding='pre')])\n",
    "    \n",
    "    predictors, label = input_sequences[:,:-1], input_sequences[:,-1]\n",
    "    label = ku.to_categorical(label,num_classes=total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model(predictors, label, max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "    model.add(LSTM(150))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(total_words,activation ='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text():\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
